<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning on dosh blog</title>
    <link>http://localhost:1313/tags/reinforcement-learning/</link>
    <description>Recent content in Reinforcement Learning on dosh blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Wed, 12 Jun 2024 01:24:48 +0900</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Policy gradient</title>
      <link>http://localhost:1313/posts/2024_06_13_policy_gradient/</link>
      <pubDate>Wed, 12 Jun 2024 01:24:48 +0900</pubDate>
      <guid>http://localhost:1313/posts/2024_06_13_policy_gradient/</guid>
      <description>policy gradient https://rail.eecs.berkeley.edu/deeprlcourse-fa18/static/slides/lec-5.pdf from
https://rail.eecs.berkeley.edu/deeprlcourse-fa18/
lilian weng policy gradient
https://lilianweng.github.io/posts/2018-04-08-policy-gradient/
The difference between policy based and value based
https://www.reddit.com/r/reinforcementlearning/comments/mkz9gl/policybased_vs_valuebased_are_they_truly_different/
$$ J(\theta) = E_{\tau ∼p_\theta(\tau)} \left[ \sum_{t}r(s_t, a_t) \right]$$ $$ \theta^* = argm\underset{\theta}ax J(\theta) $$ Object function \( J(\theta)\) is the expected return of a policy parameterized by \( \theta \). \( \tau∼p_\theta(\tau) \) means that the trajectory \( \tau \) is sampled from the distribution \( p_\theta(\tau) \). How to update the policy parameters \( \theta \) ?</description>
    </item>
    <item>
      <title>The convergence of the iteration of optimal bellman equation</title>
      <link>http://localhost:1313/posts/2024_01_15_optimal_bellman_equation/</link>
      <pubDate>Sun, 11 Feb 2024 01:24:48 +0900</pubDate>
      <guid>http://localhost:1313/posts/2024_01_15_optimal_bellman_equation/</guid>
      <description>Bellman equation plays a vital role in reinforcement learning. Iterative Policy Evaluation $$$$ $$ V_{k&amp;#43;1}(s) = \sum_{a, s&amp;#39;} \pi(a|s) p(s&amp;#39;|s,a) \{ r(s, a, s&amp;#39;) &amp;#43; \gamma V_{k}(s&amp;#39;) \}$$ $$$$ \begin{flalign} &amp;amp;V_{k}:\text{Value function after } k \text{ th iteration.}\ &amp;amp; \end{flalign} \begin{flalign} &amp;amp; \pi : \text{Policy. The probability of performing action } a \text{ under state } s. \ &amp;amp; \end{flalign} \begin{flalign} &amp;amp; p: \text{Probability of the next state } s&amp;#39; \text{ under state } s \text{ and action } a.</description>
    </item>
  </channel>
</rss>
